{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def modeldefine():\n",
    "  model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "  emotions = load_dataset('SetFit/emotion')\n",
    "  def tokenize(batch):\n",
    "      return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "  emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "  emotions_encoded.set_format('tf',columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "  BATCH_SIZE = 64\n",
    "  def order(inp):\n",
    "      '''\n",
    "      This function will group all the inputs of BERT\n",
    "      into a single dictionary and then output it with\n",
    "      labels.\n",
    "      '''\n",
    "      data = list(inp.values())\n",
    "      return {\n",
    "          'input_ids': data[1],\n",
    "          'attention_mask': data[2],\n",
    "          'token_type_ids': data[3]\n",
    "      }, data[0]\n",
    "  # converting train split of `emotions_encoded` to tensorflow format\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices(emotions_encoded['train'][:])\n",
    "  # set batch_size and shuffle\n",
    "  train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\n",
    "  # map the `order` function\n",
    "  train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "  # ... doing the same for test set ...\n",
    "  test_dataset = tf.data.Dataset.from_tensor_slices(emotions_encoded['test'][:])\n",
    "  test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "  test_dataset = test_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  class BERTForClassification(tf.keras.Model):\n",
    "\n",
    "      def __init__(self, bert_model, num_classes):\n",
    "          super().__init__()\n",
    "          self.bert = bert_model\n",
    "          self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "      def call(self, inputs):\n",
    "          x = self.bert(inputs)[1]\n",
    "          return self.fc(x)\n",
    "  inp, out = next(iter(train_dataset)) # a batch from train_dataset\n",
    "  print(inp, '\\n\\n', out)\n",
    "  classifier = BERTForClassification(model, num_classes=6)\n",
    "  classifier.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "  history = classifier.fit(\n",
    "      train_dataset,\n",
    "      epochs=3)\n",
    "  train_pred=classifier.evaluate(train_dataset)[1]\n",
    "  print(\"the training acc prediction value is\",float(train_pred*100))\n",
    "  classifier.summary()\n",
    "  pred=classifier.evaluate(test_dataset)[1]\n",
    "  print(\"the final prediction value is\",float(pred*100))\n",
    "  classifier.predict()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
