# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17tHL7bSIvwufYnoZkAxPCv7y2yH92gMT

IMPORTING LIBRARIES
"""

import pandas as pd
import numpy as np
import re
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('ggplot')

import nltk
nltk.download('stopwords')
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
from nltk import pos_tag
from nltk.corpus import words
from textblob import TextBlob

"""EDA , DATA CLEANING AND DATA PREPROCESSING"""

#opening datafile
df = pd.read_csv("train.csv")

#checking first ten data
df.head(10)

#checking no of rows
df.shape

#checking column datatype
df.dtypes

#checking data info
df.info()

#checking any null or empty rows
df.isna()

#checking any null or empty rows
df.isnull().sum()

#removing nan values
df.dropna(inplace=True)

df.isna().sum()

#checking any duplicates
df.duplicated().sum()

#checking different sentiment values
df["sentiment"].value_counts(normalize=True)

"""#observation : we have unbalanced dataset"""

#presenting sentiments in a graph to get a better understanding
graph_one=df['sentiment'].value_counts().plot(kind='bar',title='Sentiment rating',figsize=(12,6))
graph_one.set_xlabel("types of emotion")
graph_one.set_ylabel("total data points")
plt.show()

"""REMOVING PUNCTUATIONS AND SYMBOLS"""

for token in df["selected_text"]:
    new_token=re.sub(r'[^\w\s]','',token)
    df["selected_text"]=df["selected_text"].replace(token,new_token)

df.head(15)

"""LOWERING THE TEXT"""

df["selected_text"]=df["selected_text"].str.lower()
df.head()

"""REMOVAL OF STOPWORDS"""

from nltk.corpus import stopwords
stopwords.words('english')

STOPWORDS = set(stopwords.words('english'))
def remove_stopwords(text):
    return " ".join([word for word in text.split() if word not in STOPWORDS])

df['selected_text'] = df['selected_text'].apply(lambda x: remove_stopwords(x))
df.head()

"""REMOVAL OF SPECIAL CHARACTERS"""

import re
def remove_spl_chars(text):
    text = re.sub('[^a-zA-Z0-9]', ' ', text)
    text = re.sub('\s+', ' ', text)
    return text

df['selected_text'] = df['selected_text'].apply(lambda x: remove_spl_chars(x))
df.head()

"""REMOVAL OF FREQUENT WORD"""

from collections import Counter
word_count = Counter()
for text in df['selected_text']:
    for word in text.split():
        word_count[word] += 1

word_count.most_common(10)

FREQUENT_WORDS = set(word for (word, wc) in word_count.most_common(3))
def remove_freq_words(text):
    return " ".join([word for word in text.split() if word not in FREQUENT_WORDS])

df['selected_text'] = df['selected_text'].apply(lambda x: remove_freq_words(x))
df.head()

"""REMOVAL OF RARE WORDS"""

RARE_WORDS = set(word for (word, wc) in word_count.most_common()[:-10:-1])
RARE_WORDS

def remove_rare_words(text):
    return " ".join([word for word in text.split() if word not in RARE_WORDS])

df['selected_text'] = df['selected_text'].apply(lambda x: remove_rare_words(x))
df.head()

"""Stemming"""

stemmer = SnowballStemmer(language='english')
#Need to add exceptions to some word , need to find aliter to solve it
df['selected_text'] = df['selected_text'].apply(lambda x: " ".join([stemmer.stem(word) for word in x.split()]))
print("\nText after stemming:")
print(df['selected_text'].head())

"""lemmatization"""

lem = WordNetLemmatizer()
df['selected_text'] = df['selected_text'].apply(lambda x: " ".join([lem.lemmatize(word) for word in x.split()]))

print("\nText after lemmatization:")
print(df['selected_text'].head())
